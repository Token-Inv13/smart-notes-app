"use client";

import { useEffect, useMemo, useRef, useState } from "react";
import type { ReactNode } from "react";
import { createPortal } from "react-dom";
import { httpsCallable } from "firebase/functions";
import { doc, onSnapshot } from "firebase/firestore";
import { ref as storageRef, uploadBytes } from "firebase/storage";
import { db, functions as fbFunctions, storage } from "@/lib/firebase";
import { invalidateAuthSession, isAuthInvalidError } from "@/lib/authInvalidation";
import { useAuth } from "@/hooks/useAuth";
import { toUserErrorMessage } from "@/lib/userError";

type Props = {
  mobileHidden?: boolean;
  renderCustomTrigger?: (args: {
    onClick: () => void;
    ariaLabel: string;
    title: string;
  }) => ReactNode;
};

type ExecuteIntentResponse = {
  intent: {
    kind: "create_todo" | "create_task" | "create_reminder" | "schedule_meeting";
    title: string;
    confidence: number;
    requiresConfirmation: boolean;
    requiresConfirmationReason?: string | null;
    remindAtIso?: string | null;
  };
  executed: boolean;
  needsClarification?: boolean;
  missingFields?: string[];
  clarificationQuestion?: string | null;
  createdCoreObjects: Array<{ type: "task" | "taskReminder" | "calendarEvent" | "todo"; id: string }>;
  message: string;
};

type VoiceFlowStep = "idle" | "listening" | "uploading" | "transcribing" | "review" | "clarify" | "executing" | "done" | "error";

function mapMicrophoneAccessError(err: unknown): string {
  if (err instanceof DOMException) {
    const name = String(err.name || "").toLowerCase();
    const message = String(err.message || "").toLowerCase();
    if (name.includes("notfound") || message.includes("requested device not found") || message.includes("device not found")) {
      return "Aucun micro disponible. V√©rifie ton appareil audio puis r√©essaie.";
    }
    if (name.includes("notallowed") || name.includes("security") || message.includes("permission") || message.includes("denied")) {
      return "Permission micro refus√©e. Autorise le micro dans ton navigateur puis recharge la page.";
    }
    if (name.includes("notreadable") || message.includes("could not start audio source")) {
      return "Le micro est indisponible (utilis√© par une autre application ou bloqu√© par le syst√®me).";
    }
    return err.message || "Impossible d‚Äôacc√©der au micro.";
  }

  if (err instanceof Error) {
    const msg = err.message.toLowerCase();
    if (msg.includes("requested device not found") || msg.includes("device not found")) {
      return "Aucun micro disponible. V√©rifie ton appareil audio puis r√©essaie.";
    }
    if (msg.includes("permission") || msg.includes("notallowed") || msg.includes("denied")) {
      return "Permission micro refus√©e. Autorise le micro dans ton navigateur puis recharge la page.";
    }
    if (msg.includes("not supported") || msg.includes("unsupported")) {
      return "Enregistrement micro non support√© sur cet appareil.";
    }
    return err.message;
  }

  if (typeof err === "string") {
    const msg = err.toLowerCase();
    if (msg.includes("requested device not found") || msg.includes("device not found")) {
      return "Aucun micro disponible. V√©rifie ton appareil audio puis r√©essaie.";
    }
    if (msg.includes("permission") || msg.includes("notallowed") || msg.includes("denied")) {
      return "Permission micro refus√©e. Autorise le micro dans ton navigateur puis recharge la page.";
    }
    return `Impossible d‚Äôacc√©der au micro: ${err}`;
  }

  return "Impossible d‚Äôacc√©der au micro.";
}

export default function VoiceAgentButton({ mobileHidden, renderCustomTrigger }: Props) {
  const { user } = useAuth();
  const [open, setOpen] = useState(false);
  const [flowStep, setFlowStep] = useState<VoiceFlowStep>("idle");
  const [elapsedMs, setElapsedMs] = useState(0);
  const [transcript, setTranscript] = useState("");
  const [clarificationInput, setClarificationInput] = useState("");
  const [error, setError] = useState<string | null>(null);
  const [result, setResult] = useState<ExecuteIntentResponse | null>(null);
  const [localPreviewHint, setLocalPreviewHint] = useState<string | null>(null);
  const [clarificationSubmitting, setClarificationSubmitting] = useState(false);
  const [jobId, setJobId] = useState<string | null>(null);
  const [resultId, setResultId] = useState<string | null>(null);

  const recorderRef = useRef<MediaRecorder | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const chunksRef = useRef<BlobPart[]>([]);
  const timerRef = useRef<number | null>(null);
  const startedAtRef = useRef<number>(0);
  const audioContextRef = useRef<AudioContext | null>(null);
  const analyserRef = useRef<AnalyserNode | null>(null);
  const sourceRef = useRef<MediaStreamAudioSourceNode | null>(null);
  const silenceIntervalRef = useRef<number | null>(null);
  const startedAutomaticallyRef = useRef(false);
  const lastVoiceMsRef = useRef<number>(0);
  const heardVoiceRef = useRef(false);
  const analyzedResultIdRef = useRef<string | null>(null);
  const noiseFloorRef = useRef(0.01);
  const calibrationUntilRef = useRef(0);
  const clarificationInputRef = useRef<HTMLInputElement | null>(null);

  const maxDurationMs = 60 * 1000;
  const silenceAutoStopMs = 1000;
  const noVoiceAutoStopMs = 7000;
  const maxBytes = 25 * 1024 * 1024;

  const buildLocalIntentPreview = (text: string): string | null => {
    const raw = text.trim();
    if (!raw) return null;
    const lower = raw.toLowerCase();
    const hasMeeting =
      lower.includes("r√©union") ||
      lower.includes("reunion") ||
      lower.includes("meeting") ||
      lower.includes("rdv") ||
      lower.includes("rendez-vous") ||
      lower.includes("agenda") ||
      lower.includes("calendrier");
    if (hasMeeting) {
      return "Pr√©-analyse rapide: demande de r√©union d√©tect√©e.";
    }

    const hasReminder =
      lower.includes("rappel") ||
      lower.includes("rappelle") ||
      lower.includes("souviens") ||
      lower.includes("n'oublie") ||
      lower.includes("n oublie");
    if (hasReminder) {
      return "Pr√©-analyse rapide: rappel d√©tect√©.";
    }

    const hasTodoKeyword =
      lower.includes("todo") ||
      lower.includes("to-do") ||
      lower.includes("checklist") ||
      lower.includes("√† faire") ||
      lower.includes("a faire") ||
      lower.includes("liste");
    const hasTaskKeyword =
      lower.includes("t√¢che") ||
      lower.includes("tache") ||
      lower.includes("task") ||
      lower.includes("projet") ||
      lower.includes("deadline") ||
      lower.includes("√©ch√©ance") ||
      lower.includes("echeance");
    const words = raw.split(/\s+/).filter(Boolean);
    const shortActionLike = words.length > 0 && words.length <= 5;
    const hasScheduleSignal = /\b(demain|ce soir|ce matin|lundi|mardi|mercredi|jeudi|vendredi|samedi|dimanche)\b/i.test(lower);

    if (hasTodoKeyword || (!hasTaskKeyword && !hasScheduleSignal && shortActionLike)) {
      return "Pr√©-analyse rapide: checklist d√©tect√©e.";
    }

    return "Pr√©-analyse rapide: √©l√©ment d‚Äôagenda d√©tect√©.";
  };

  const isCallableUnauthenticated = (err: unknown) => {
    if (isAuthInvalidError(err)) return true;
    const code = typeof (err as { code?: unknown })?.code === "string" ? String((err as { code?: unknown }).code) : "";
    return code.includes("unauthenticated");
  };

  const clearTimer = () => {
    if (timerRef.current) window.clearInterval(timerRef.current);
    timerRef.current = null;
  };

  const clearSilenceWatcher = () => {
    if (silenceIntervalRef.current) window.clearInterval(silenceIntervalRef.current);
    silenceIntervalRef.current = null;
  };

  const cleanupAudioGraph = () => {
    try {
      sourceRef.current?.disconnect();
    } catch {
      // ignore
    }
    sourceRef.current = null;
    analyserRef.current = null;
    try {
      void audioContextRef.current?.close();
    } catch {
      // ignore
    }
    audioContextRef.current = null;
  };

  const cleanupStream = () => {
    try {
      streamRef.current?.getTracks().forEach((t) => t.stop());
    } catch {
      // ignore
    }
    streamRef.current = null;
  };

  const hardStopRecordingResources = () => {
    clearTimer();
    clearSilenceWatcher();
    cleanupAudioGraph();
    cleanupStream();
  };

  const createVoiceJob = async () => {
    const fn = httpsCallable<{ mode?: string }, { jobId: string; storagePath: string }>(fbFunctions, "assistantCreateVoiceJob");
    const res = await fn({ mode: "standalone" });
    return res.data;
  };

  const requestTranscription = async (id: string) => {
    const fn = httpsCallable<{ jobId: string }, { jobId: string; resultId: string }>(fbFunctions, "assistantRequestVoiceTranscription");
    const res = await fn({ jobId: id });
    return res.data;
  };

  const runIntent = async (execute: boolean, transcriptInput?: string) => {
    const effectiveTranscript = (transcriptInput ?? transcript).trim();
    if (!effectiveTranscript) return;

    setFlowStep(execute ? "executing" : "transcribing");
    setError(null);
    if (!execute) {
      setLocalPreviewHint(buildLocalIntentPreview(effectiveTranscript));
    }

    try {
      const fn = httpsCallable<{ transcript: string; execute: boolean }, ExecuteIntentResponse>(fbFunctions, "assistantExecuteIntent");
      const res = await fn({ transcript: effectiveTranscript, execute });
      setResult(res.data);
      setLocalPreviewHint(null);

      if (execute && res.data.executed) {
        setFlowStep("done");
        return;
      }
      if (res.data.needsClarification) {
        setFlowStep("clarify");
        return;
      }
      setFlowStep("review");
    } catch (e) {
      if (isCallableUnauthenticated(e)) {
        void invalidateAuthSession();
        return;
      }
      setError(toUserErrorMessage(e, "Impossible d‚Äôex√©cuter la commande vocale."));
      setFlowStep("error");
    }
  };

  const processBlob = async (blob: Blob) => {
    if (!user?.uid) {
      setError("Tu dois √™tre connect√©.");
      setFlowStep("error");
      return;
    }
    if (blob.size <= 0) {
      setError("Audio vide.");
      setFlowStep("error");
      return;
    }
    if (blob.size > maxBytes) {
      setError("Audio trop volumineux (max 25MB).");
      setFlowStep("error");
      return;
    }

    setFlowStep("uploading");

    try {
      const created = await createVoiceJob();
      setJobId(created.jobId);
      const fileRef = storageRef(storage, created.storagePath);
      await uploadBytes(fileRef, blob, { contentType: blob.type || "audio/webm" });
      setFlowStep("transcribing");
      const tr = await requestTranscription(created.jobId);
      setResultId(tr.resultId);
    } catch (e) {
      if (isCallableUnauthenticated(e)) {
        void invalidateAuthSession();
        return;
      }
      setError(toUserErrorMessage(e, "Impossible de traiter l‚Äôaudio."));
      setFlowStep("error");
    }
  };

  const stopListening = () => {
    const recorder = recorderRef.current;
    if (!recorder || recorder.state === "inactive") return;
    try {
      recorder.stop();
    } catch {
      // ignore
    }
  };

  const startListening = async () => {
    if (flowStep === "listening" || flowStep === "uploading" || flowStep === "transcribing" || flowStep === "executing") return;
    if (typeof window === "undefined" || typeof window.MediaRecorder === "undefined" || !navigator.mediaDevices?.getUserMedia) {
      setError("Enregistrement non support√© sur cet appareil.");
      setFlowStep("error");
      return;
    }

    setError(null);
    setResult(null);
    setClarificationInput("");
    setTranscript("");
    setLocalPreviewHint(null);
    setElapsedMs(0);
    setJobId(null);
    setResultId(null);
    analyzedResultIdRef.current = null;
    noiseFloorRef.current = 0.01;
    calibrationUntilRef.current = Date.now() + 1200;

    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      streamRef.current = stream;
      chunksRef.current = [];
      heardVoiceRef.current = false;
      lastVoiceMsRef.current = Date.now();

      const recorder = (() => {
        try {
          return new MediaRecorder(stream, { mimeType: "audio/webm;codecs=opus" });
        } catch {
          return new MediaRecorder(stream);
        }
      })();
      recorderRef.current = recorder;

      recorder.ondataavailable = (e) => {
        if (e.data && e.data.size > 0) {
          chunksRef.current.push(e.data);
        }
      };

      recorder.onstop = () => {
        clearTimer();
        clearSilenceWatcher();
        cleanupAudioGraph();
        cleanupStream();
        const blob = new Blob(chunksRef.current, { type: recorder.mimeType || "audio/webm" });
        chunksRef.current = [];
        recorderRef.current = null;
        void processBlob(blob);
      };

      recorder.start(350);
      setFlowStep("listening");
      startedAtRef.current = Date.now();

      timerRef.current = window.setInterval(() => {
        const elapsed = Date.now() - startedAtRef.current;
        setElapsedMs(elapsed);
        if (elapsed >= maxDurationMs) {
          stopListening();
        }
      }, 200);

      const audioCtx = new AudioContext();
      const analyser = audioCtx.createAnalyser();
      analyser.fftSize = 2048;
      const source = audioCtx.createMediaStreamSource(stream);
      source.connect(analyser);

      audioContextRef.current = audioCtx;
      analyserRef.current = analyser;
      sourceRef.current = source;

      silenceIntervalRef.current = window.setInterval(() => {
        const an = analyserRef.current;
        if (!an) return;
        const arr = new Uint8Array(an.fftSize);
        an.getByteTimeDomainData(arr);
        let sum = 0;
        for (let i = 0; i < arr.length; i += 1) {
          const sample = arr[i] ?? 128;
          const v = (sample - 128) / 128;
          sum += v * v;
        }
        const rms = Math.sqrt(sum / arr.length);
        const now = Date.now();
        if (now < calibrationUntilRef.current) {
          noiseFloorRef.current = Math.max(0.005, noiseFloorRef.current * 0.9 + rms * 0.1);
        } else {
          noiseFloorRef.current = Math.max(0.005, noiseFloorRef.current * 0.98 + rms * 0.02);
        }

        const dynamicThreshold = Math.max(0.018, noiseFloorRef.current * 2.8);
        const speaking = rms > dynamicThreshold;
        if (speaking) {
          heardVoiceRef.current = true;
          lastVoiceMsRef.current = now;
        }

        if (!heardVoiceRef.current && now - startedAtRef.current > noVoiceAutoStopMs) {
          stopListening();
          return;
        }

        if (heardVoiceRef.current && now - lastVoiceMsRef.current > silenceAutoStopMs) {
          stopListening();
        }
      }, 180);
    } catch (e) {
      setError(mapMicrophoneAccessError(e));
      setFlowStep("error");
      hardStopRecordingResources();
    }
  };

  const closeModal = () => {
    setOpen(false);
    startedAutomaticallyRef.current = false;
    hardStopRecordingResources();
    try {
      recorderRef.current?.stop();
    } catch {
      // ignore
    }
    recorderRef.current = null;
  };

  const retryVoice = () => {
    setResult(null);
    setError(null);
    setTranscript("");
    setClarificationInput("");
    setClarificationSubmitting(false);
    setFlowStep("idle");
    void startListening();
  };

  const parseClarificationTimeInput = (input: string): { normalized: string; shortLabel: string; displayLabel: string } | null => {
    const compact = input.trim().replace(/\s+/g, "");
    if (!compact) return null;

    const hourOnly = /^([01]?\d|2[0-3])$/.exec(compact);
    if (hourOnly) {
      const hour = Number(hourOnly[1]);
      return {
        normalized: `${hour}h`,
        shortLabel: `${hour}h`,
        displayLabel: `${hour.toString().padStart(2, "0")}:00`,
      };
    }

    const hourMinute = /^([01]?\d|2[0-3])(?::|h|\.)([0-5]\d)$/.exec(compact);
    if (hourMinute) {
      const hour = Number(hourMinute[1]);
      const minute = hourMinute[2];
      return {
        normalized: `${hour}h${minute}`,
        shortLabel: `${hour}h${minute}`,
        displayLabel: `${hour.toString().padStart(2, "0")}:${minute}`,
      };
    }

    return null;
  };

  const normalizeClarificationTimeInput = (input: string) => {
    const parsed = parseClarificationTimeInput(input);
    if (parsed) return parsed.normalized;

    return input.trim();
  };

  const applyClarification = async () => {
    if (clarificationSubmitting) return;
    const extra = normalizeClarificationTimeInput(clarificationInput);
    if (!extra) return;
    const merged = `${transcript.trim()} ${extra}`.trim();
    setTranscript(merged);
    setClarificationInput("");
    setError(null);
    setClarificationSubmitting(true);
    try {
      await runIntent(true, merged);
    } finally {
      setClarificationSubmitting(false);
    }
  };

  useEffect(() => {
    if (!open) return;
    if (startedAutomaticallyRef.current) return;
    startedAutomaticallyRef.current = true;
    void startListening();
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, [open]);

  useEffect(() => {
    if (!open) return;
    if (!(flowStep === "clarify" || result?.needsClarification === true)) return;
    const id = window.setTimeout(() => {
      clarificationInputRef.current?.focus();
    }, 0);
    return () => window.clearTimeout(id);
  }, [flowStep, open, result?.needsClarification]);

  useEffect(() => {
    if (!open) return;
    return () => {
      hardStopRecordingResources();
    };
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, [open]);

  useEffect(() => {
    if (!user?.uid || !jobId) return;
    const jobRef = doc(db, "users", user.uid, "assistantVoiceJobs", jobId);
    const unsub = onSnapshot(jobRef, (snap) => {
      const d = snap.exists() ? (snap.data() as { status?: unknown; errorMessage?: unknown }) : null;
      if (d?.status === "error" && typeof d.errorMessage === "string") {
        setError(d.errorMessage);
        setFlowStep("error");
      }
    });
    return () => unsub();
  }, [jobId, user?.uid]);

  useEffect(() => {
    if (!user?.uid || !resultId) return;
    const resultRef = doc(db, "users", user.uid, "assistantVoiceResults", resultId);
    const unsub = onSnapshot(resultRef, (snap) => {
      if (analyzedResultIdRef.current === resultId) return;
      const d = snap.exists() ? (snap.data() as { transcript?: unknown }) : null;
      if (typeof d?.transcript !== "string" || !d.transcript.trim()) return;
      const nextTranscript = d.transcript.trim();
      analyzedResultIdRef.current = resultId;
      setTranscript(nextTranscript);
      setFlowStep("transcribing");
      void runIntent(false, nextTranscript);
    });
    return () => unsub();
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, [resultId, user?.uid]);

  const parsedHint = useMemo(() => {
    if (!result) return null;
    const intent = result.intent;
    const conf = `${Math.round((intent.confidence ?? 0) * 100)}%`;
    if (intent.kind === "create_todo") return `Compris: cr√©er une checklist ‚Äî ‚Äú${intent.title}‚Äù (${conf}).`;
    if (intent.kind === "create_task") return `Compris: cr√©er un √©l√©ment d‚Äôagenda ‚Äî ‚Äú${intent.title}‚Äù (${conf}).`;
    if (intent.kind === "create_reminder") {
      return `Compris: cr√©er un rappel ‚Äî ‚Äú${intent.title}‚Äù${intent.remindAtIso ? ` √† ${new Date(intent.remindAtIso).toLocaleString("fr-FR")}` : ""} (${conf}).`;
    }
    return `Compris: planifier une r√©union ‚Äî ‚Äú${intent.title}‚Äù (${conf}).`;
  }, [result]);

  const displayedHint = parsedHint ?? localPreviewHint;
  const clarificationPending = flowStep === "clarify" || result?.needsClarification === true;
  const isBusyStep = flowStep === "listening" || flowStep === "uploading" || flowStep === "transcribing" || flowStep === "executing";
  const clarificationButtonDisabled = !clarificationInput.trim() || clarificationSubmitting;
  const parsedClarificationTime = useMemo(() => parseClarificationTimeInput(clarificationInput), [clarificationInput]);
  const clarificationActionLabel = useMemo(() => {
    if (clarificationSubmitting) return "Traitement...";
    if (!parsedClarificationTime) return "OK";
    const at = parsedClarificationTime.shortLabel;
    if (result?.intent.kind === "schedule_meeting") return `Cr√©er la r√©union √† ${at}`;
    if (result?.intent.kind === "create_task") return `Cr√©er l‚Äô√©l√©ment d‚Äôagenda √† ${at}`;
    return `Cr√©er le rappel √† ${at}`;
  }, [clarificationSubmitting, parsedClarificationTime, result?.intent.kind]);
  const confirmationHint =
    flowStep === "review" && result?.intent?.requiresConfirmation
      ? "Derni√®re √©tape: appuie sur Oui pour confirmer la cr√©ation."
      : null;

  const stepHint = useMemo(() => {
    if (flowStep === "listening") return "Je t‚Äô√©coute‚Ä¶ parle naturellement.";
    if (flowStep === "uploading") return "Envoi de l‚Äôaudio‚Ä¶";
    if (flowStep === "transcribing") return "Transcription et analyse en cours‚Ä¶";
    if (flowStep === "executing") return "Ex√©cution de l‚Äôaction‚Ä¶";
    if (flowStep === "done") return "Action termin√©e ‚úÖ";
    if (flowStep === "clarify") return result?.clarificationQuestion ?? "Ajoute l'heure puis appuie sur OK pour cr√©er.";
    if (flowStep === "review") return "Voici mon interpr√©tation. Je lance l‚Äôaction ?";
    return "Appuie sur le micro pour commencer.";
  }, [flowStep, result?.clarificationQuestion]);

  const customTrigger = renderCustomTrigger
    ? renderCustomTrigger({
        onClick: () => setOpen(true),
        ariaLabel: "Assistant vocal",
        title: "Assistant vocal",
      })
    : null;

  const voiceModal = open ? (
    <div
      className="fixed inset-0 z-[70] bg-black/45 p-0 md:p-4 flex items-end md:items-center md:justify-center"
      role="dialog"
      aria-modal="true"
      aria-label="Assistant vocal"
      onMouseDown={(e) => {
        if (e.target === e.currentTarget) closeModal();
      }}
    >
      <div
        className="w-full md:max-w-[440px] rounded-t-2xl md:rounded-xl border border-border bg-card p-3 md:p-4 space-y-2 md:space-y-3 shadow-2xl max-h-[82vh] overflow-y-auto"
        onMouseDown={(e) => e.stopPropagation()}
      >
        <div className="flex items-center justify-between gap-3">
          <div className="text-sm font-semibold">Assistant vocal</div>
          <button type="button" className="text-sm text-muted-foreground" onClick={closeModal}>
            Fermer
          </button>
        </div>

        <div className="flex flex-col items-center justify-center gap-2 py-1 md:py-2">
          <button
            type="button"
            onClick={() => {
              if (flowStep === "listening") {
                stopListening();
              } else {
                void startListening();
              }
            }}
            className={
              "relative h-20 w-20 md:h-24 md:w-24 rounded-full border border-primary/40 text-2xl md:text-3xl flex items-center justify-center transition-all " +
              (flowStep === "listening" ? "bg-primary/20 animate-pulse" : "bg-primary/10")
            }
            aria-label={flowStep === "listening" ? "Stopper l‚Äô√©coute" : "D√©marrer l‚Äô√©coute"}
            title={flowStep === "listening" ? "Stop" : "Parler"}
          >
            üé§
          </button>
          {flowStep === "listening" ? <div className="text-xs text-muted-foreground">{Math.max(0, Math.floor(elapsedMs / 1000))}s</div> : null}
          <div className="text-xs text-muted-foreground text-center">{stepHint}</div>
        </div>

        {transcript ? (
          <div className="rounded-md border border-border bg-background p-2.5 md:p-3 text-sm whitespace-pre-wrap max-h-24 overflow-y-auto">{transcript}</div>
        ) : null}

        {clarificationPending ? (
          <form
            className="space-y-2"
            onSubmit={(e) => {
              e.preventDefault();
              void applyClarification();
            }}
          >
            <input
              ref={clarificationInputRef}
              type="text"
              className="w-full rounded-md border border-input bg-background px-3 py-2 text-sm"
              autoFocus
              placeholder="Ex: 9, 9h, 9h30"
              value={clarificationInput}
              onChange={(e) => setClarificationInput(e.target.value)}
            />
            {parsedClarificationTime ? <div className="text-xs text-muted-foreground">Heure d√©tect√©e : {parsedClarificationTime.displayLabel}</div> : null}
          </form>
        ) : null}

        <div className="flex flex-wrap items-center gap-2 pt-1">
          {flowStep === "listening" ? (
            <button
              type="button"
              className="px-3 py-2 rounded-md border border-input text-sm"
              onClick={stopListening}
            >
              STOP
            </button>
          ) : null}

          {flowStep === "review" && !clarificationPending ? (
            <>
              <button
                type="button"
                className="px-3 py-2 rounded-md bg-primary text-primary-foreground text-sm"
                onClick={() => void runIntent(true)}
              >
                Oui
              </button>
              <button
                type="button"
                className="px-3 py-2 rounded-md border border-input text-sm"
                onClick={retryVoice}
              >
                Non
              </button>
              <button
                type="button"
                className="px-3 py-2 rounded-md border border-input text-sm"
                onClick={closeModal}
              >
                Annuler
              </button>
            </>
          ) : null}

          {clarificationPending ? (
            <>
              <button
                type="button"
                className="px-3 py-2 rounded-md bg-primary text-primary-foreground text-sm disabled:opacity-50"
                disabled={clarificationButtonDisabled}
                onClick={() => void applyClarification()}
              >
                {clarificationActionLabel}
              </button>
              <button
                type="button"
                className="px-3 py-2 rounded-md border border-border bg-muted/40 text-muted-foreground text-sm"
                onClick={closeModal}
              >
                Annuler
              </button>
            </>
          ) : null}

          {flowStep === "done" ? (
            <button
              type="button"
              className="px-3 py-2 rounded-md bg-primary text-primary-foreground text-sm"
              onClick={closeModal}
            >
              OK
            </button>
          ) : null}

          {flowStep === "error" ? (
            <>
              <button
                type="button"
                className="px-3 py-2 rounded-md border border-input text-sm"
                onClick={retryVoice}
              >
                R√©essayer
              </button>
              <button
                type="button"
                className="px-3 py-2 rounded-md border border-input text-sm"
                onClick={closeModal}
              >
                Annuler
              </button>
            </>
          ) : null}

          {flowStep === "idle" ? (
            <button
              type="button"
              className="px-3 py-2 rounded-md border border-input text-sm"
              onClick={() => void startListening()}
            >
              Parler
            </button>
          ) : null}

          <button
            type="button"
            className="px-3 py-2 rounded-md border border-border bg-muted/40 text-muted-foreground text-sm disabled:opacity-50"
            disabled={isBusyStep}
            onClick={retryVoice}
          >
            Nouvelle commande
          </button>
        </div>

        {displayedHint ? <div className="text-sm">{displayedHint}</div> : null}
        {result?.message ? <div className="text-xs text-muted-foreground">{result.message}</div> : null}
        {confirmationHint ? <div className="text-xs text-amber-600">{confirmationHint}</div> : null}
        {error ? <div className="text-xs text-destructive">{error}</div> : null}
      </div>
    </div>
  ) : null;

  return (
    <>
      {renderCustomTrigger ? (
        customTrigger
      ) : (
        <>
          <button
            type="button"
            className="hidden md:inline-flex fixed right-8 bottom-8 z-50 h-12 w-12 items-center justify-center rounded-full bg-primary text-primary-foreground shadow-lg"
            onClick={() => setOpen(true)}
            aria-label="Assistant vocal"
            title="Assistant vocal"
          >
            üé§
          </button>

          {!mobileHidden ? (
            <button
              type="button"
              className="md:hidden fixed right-4 bottom-[calc(5rem+env(safe-area-inset-bottom))] z-50 h-11 w-11 items-center justify-center rounded-full bg-primary text-primary-foreground shadow-lg"
              onClick={() => setOpen(true)}
              aria-label="Assistant vocal"
              title="Assistant vocal"
            >
              üé§
            </button>
          ) : null}
        </>
      )}

      {voiceModal ? (typeof document !== "undefined" ? createPortal(voiceModal, document.body) : voiceModal) : null}
    </>
  );
}
